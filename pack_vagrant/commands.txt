
Vagrant 1.9.1
Virtualbox 5.1.10

vagrant global-status
vagrant global-status --prune

sudo kubeadm init --token c52ddf.f9324a7fa5058c6f --api-advertise-addresses=172.17.4.101
sudo kubeadm join --token c52ddf.f9324a7fa5058c6f 172.17.4.101


ansible-playbook -i inventory_vagrant --key-file ~/.vagrant.d/insecure_private_key playbooks/test-task.yml


PROBLEMS

default ethernet card wrong...

need 2 workers for galaxy (gluster problem?)

TODOs

Fix inventory file



printf '[Service]\nEnvironment="KUBELET_EXTRA_ARGS=--hostname-override=%s"\n' ${api_advertise_addresses} > /etc/systemd/system/kubelet.service.d/09-hostname-override.conf
printf '[Service]\nEnvironment="KUBELET_EXTRA_ARGS=--hostname-override=%s"\n' ${api_advertise_addresses} > /etc/systemd/system/kubelet.service.d/15-hostname-override.conf
systemctl daemon-reload
systemctl restart kubelet

setup galaxy: 2min 25sek

setup galaxy: local_path 1min 29sek

setup galaxy: nfs 1min 27sek

## Install nfs (Ubuntu 16.04):

# install files
sudo apt-get install nfs-kernel-server

# create shared dir (for example /shared/data)
sudo mkdir -p /shared/data

# set owner to dir
sudo chown nobody:nogroup /shared/data

# add shared directory to nfs-exports config (/etc/exports): 
# you need to put the subnet of the servers that should access the data
/shared      192.168.10.0/24(rw,fsid=0,insecure,no_subtree_check,async,no_root_squash)
/shared/data 192.168.10.0/24(rw,nohide,insecure,no_subtree_check,async,no_root_squash)

# reload config
sudo exportfs -a

# restart server
sudo service nfs-kernel-server start

# find network card
ifconfig

# find gateway
netstat -nr

vb.customize ['modifyvm', :id, '--natnet1', '192.168.103.0/24']

  #worker.customize ["modifyvm", :id, "--natnet1", "192.168.15/24"]
  #-netname natnet1 --network "192.168.15.0/24" --enable --dhcp on

  #config.vm.synced_folder "/tmp/vagrant_share",
  #                       "/vagrant_share",
  #                        type: "nfs",
   #                     :linux__nfs_options => ["rw","nohide","no_root_squash","insecure","no_subtree_check","async"]



ansible 2.2.0.0
Vagrant 1.9.1
Virtualbox 5.1

git clone https://github.com/mcapuccini/KubeNow.git
cd KubeNow
git checkout feature/local_vagrant

# before you do vagrant up you need to edit some parameters in the Vagrantfile


# you will need to know the network gateway of your host:
netstat -nr

# now edit the Vagrantfile

# then vagrant up
vagrant up

# Before you deploy with ansible you need to add the ssh key of the virtual machines to your keyring
ssh-add ~/.vagrant.d/boxes/kubenow/0/virtualbox/vagrant_private_key

# now you kan provision the core functionality with ansible
ansible-playbook -i inventory -e "nodes_count=3" --skip-tags "cloudflare" playbooks/install-core.yml

